{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[1]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[1]:\n",
      " [0.1 0.1 0.1 0.1]\n",
      "W[2]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[2]:\n",
      " [0.1 0.1 0.1]\n",
      "W[3]:\n",
      " [[0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]]\n",
      "b[3]:\n",
      " [0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "L = len(units) - 1\n",
    "\n",
    "# Initialize weights with correct shapes\n",
    "W = [None]\n",
    "b = [None]\n",
    "\n",
    "for l in range(1, L+1):\n",
    "    W.append(np.ones((units[l], units[l-1])) * 0.1)\n",
    "    print(f\"W[{l}]:\\n\", W[l])\n",
    "    b.append(np.ones((units[l])) * 0.1)\n",
    "    print(f\"b[{l}]:\\n\", b[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h[0] (4,):\n",
      " [1 0 1 0]\n",
      "z[1] (4,):\n",
      " [0.3 0.3 0.3 0.3]\n",
      "h[1] (4,):\n",
      " [0.29131261 0.29131261 0.29131261 0.29131261]\n",
      "z[2] (3,):\n",
      " [0.21652504 0.21652504 0.21652504]\n",
      "h[2] (3,):\n",
      " [0.21320353 0.21320353 0.21320353]\n",
      "z[3] (3,):\n",
      " [0.16396106 0.16396106 0.16396106]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "z = [None]\n",
    "h = [inputs[0]]\n",
    "print(f\"h[0] {h[0].shape}:\\n\", h[0])\n",
    "\n",
    "for l in range(1, len(units)):\n",
    "    z.append(W[l] @ h[l-1] + b[l])\n",
    "    print(f\"z[{l}] {z[l].shape}:\\n\", z[l])\n",
    "\n",
    "    if l == L:\n",
    "        break\n",
    "\n",
    "    h.append(np.tanh(z[l]))\n",
    "    print(f\"h[{l}] {h[l].shape}:\\n\", h[l])\n",
    "\n",
    "y_hat = z[L]\n",
    "y = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.37636378397755565\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "loss = (1/2) * (y_hat - y) @ (y_hat - y)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dz[3] (3,):\n",
      " [ 0.16396106 -0.83603894  0.16396106]\n",
      "dL/dW[3] (3, 3):\n",
      " [[ 0.03495708  0.03495708  0.03495708]\n",
      " [-0.17824646 -0.17824646 -0.17824646]\n",
      " [ 0.03495708  0.03495708  0.03495708]]\n",
      "dL/db[3] (3,):\n",
      " [ 0.16396106 -0.83603894  0.16396106]\n",
      "dL/dh[2] (3,):\n",
      " [-0.05081168 -0.05081168 -0.05081168]\n",
      "dL/dz[2] (3,):\n",
      " [-0.048502 -0.048502 -0.048502]\n",
      "dL/dW[2] (3, 4):\n",
      " [[-0.01412924 -0.01412924 -0.01412924 -0.01412924]\n",
      " [-0.01412924 -0.01412924 -0.01412924 -0.01412924]\n",
      " [-0.01412924 -0.01412924 -0.01412924 -0.01412924]]\n",
      "dL/db[2] (3,):\n",
      " [-0.048502 -0.048502 -0.048502]\n",
      "dL/dh[1] (4,):\n",
      " [-0.0145506 -0.0145506 -0.0145506 -0.0145506]\n",
      "dL/dz[1] (4,):\n",
      " [-0.01331579 -0.01331579 -0.01331579 -0.01331579]\n",
      "dL/dW[1] (4, 4):\n",
      " [[-0.01331579 -0.         -0.01331579 -0.        ]\n",
      " [-0.01331579 -0.         -0.01331579 -0.        ]\n",
      " [-0.01331579 -0.         -0.01331579 -0.        ]\n",
      " [-0.01331579 -0.         -0.01331579 -0.        ]]\n",
      "dL/db[1] (4,):\n",
      " [-0.01331579 -0.01331579 -0.01331579 -0.01331579]\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "dL_dz = [None] * (L+1)\n",
    "dL_dW = [None] * (L+1)\n",
    "dL_db = [None] * (L+1)\n",
    "dL_dh = [None] * L\n",
    "\n",
    "for l in range(L, 0, -1):\n",
    "    if l == L:\n",
    "        dL_dz[L] = y_hat - y\n",
    "    \n",
    "    else:\n",
    "        dL_dh[l] = W[l+1].T @ dL_dz[l+1]\n",
    "        print(f\"dL/dh[{l}] {dL_dh[l].shape}:\\n\", dL_dh[l])\n",
    "\n",
    "        dL_dz[l] = dL_dh[l] * (1 - h[l]**2)\n",
    "    print(f\"dL/dz[{l}] {dL_dz[l].shape}:\\n\", dL_dz[l])\n",
    "\n",
    "    dL_dW[l] = np.outer(dL_dz[l], h[l-1])\n",
    "    print(f\"dL/dW[{l}] {dL_dW[l].shape}:\\n\", dL_dW[l])\n",
    "\n",
    "    dL_db[l] = dL_dz[l]\n",
    "    print(f\"dL/db[{l}] {dL_db[l].shape}:\\n\", dL_db[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[1]:\n",
      " [[0.10133158 0.1        0.10133158 0.1       ]\n",
      " [0.10133158 0.1        0.10133158 0.1       ]\n",
      " [0.10133158 0.1        0.10133158 0.1       ]\n",
      " [0.10133158 0.1        0.10133158 0.1       ]]\n",
      "b[1]:\n",
      " [0.10133158 0.10133158 0.10133158 0.10133158]\n",
      "W[2]:\n",
      " [[0.10141292 0.10141292 0.10141292 0.10141292]\n",
      " [0.10141292 0.10141292 0.10141292 0.10141292]\n",
      " [0.10141292 0.10141292 0.10141292 0.10141292]]\n",
      "b[2]:\n",
      " [0.1048502 0.1048502 0.1048502]\n",
      "W[3]:\n",
      " [[0.09650429 0.09650429 0.09650429]\n",
      " [0.11782465 0.11782465 0.11782465]\n",
      " [0.09650429 0.09650429 0.09650429]]\n",
      "b[3]:\n",
      " [0.08360389 0.18360389 0.08360389]\n"
     ]
    }
   ],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "for l in range(1, L+1):\n",
    "    W[l] -= eta * dL_dW[l]\n",
    "    print(f\"W[{l}]:\\n\", W[l])\n",
    "\n",
    "    b[l] -= eta * dL_db[l]\n",
    "    print(f\"b[{l}]:\\n\", b[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[1]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[1]:\n",
      " [0.1 0.1 0.1 0.1]\n",
      "W[2]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[2]:\n",
      " [0.1 0.1 0.1]\n",
      "W[3]:\n",
      " [[0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]]\n",
      "b[3]:\n",
      " [0.1 0.1 0.1]\n",
      "h[0] (4,):\n",
      " [1 0 1 0]\n",
      "z[1] (4,):\n",
      " [0.3 0.3 0.3 0.3]\n",
      "h[1] (4,):\n",
      " [0.29131261 0.29131261 0.29131261 0.29131261]\n",
      "z[2] (3,):\n",
      " [0.21652504 0.21652504 0.21652504]\n",
      "h[2] (3,):\n",
      " [0.21320353 0.21320353 0.21320353]\n",
      "z[3] (3,):\n",
      " [0.16396106 0.16396106 0.16396106]\n",
      "Loss: 1.0986122886681098\n",
      "dL/dz[3] (3,):\n",
      " [ 0.33333333 -0.66666667  0.33333333]\n",
      "dL/dW[3] (3, 3):\n",
      " [[ 0.07106784  0.07106784  0.07106784]\n",
      " [-0.14213569 -0.14213569 -0.14213569]\n",
      " [ 0.07106784  0.07106784  0.07106784]]\n",
      "dL/db[3] (3,):\n",
      " [ 0.33333333 -0.66666667  0.33333333]\n",
      "dL/dh[2] (3,):\n",
      " [-1.38777878e-17 -1.38777878e-17 -1.38777878e-17]\n",
      "dL/dz[2] (3,):\n",
      " [-1.32469626e-17 -1.32469626e-17 -1.32469626e-17]\n",
      "dL/dW[2] (3, 4):\n",
      " [[-3.85900728e-18 -3.85900728e-18 -3.85900728e-18 -3.85900728e-18]\n",
      " [-3.85900728e-18 -3.85900728e-18 -3.85900728e-18 -3.85900728e-18]\n",
      " [-3.85900728e-18 -3.85900728e-18 -3.85900728e-18 -3.85900728e-18]]\n",
      "dL/db[2] (3,):\n",
      " [-1.32469626e-17 -1.32469626e-17 -1.32469626e-17]\n",
      "dL/dh[1] (4,):\n",
      " [-3.97408878e-18 -3.97408878e-18 -3.97408878e-18 -3.97408878e-18]\n",
      "dL/dz[1] (4,):\n",
      " [-3.63683553e-18 -3.63683553e-18 -3.63683553e-18 -3.63683553e-18]\n",
      "dL/dW[1] (4, 4):\n",
      " [[-3.63683553e-18 -0.00000000e+00 -3.63683553e-18 -0.00000000e+00]\n",
      " [-3.63683553e-18 -0.00000000e+00 -3.63683553e-18 -0.00000000e+00]\n",
      " [-3.63683553e-18 -0.00000000e+00 -3.63683553e-18 -0.00000000e+00]\n",
      " [-3.63683553e-18 -0.00000000e+00 -3.63683553e-18 -0.00000000e+00]]\n",
      "dL/db[1] (4,):\n",
      " [-3.63683553e-18 -3.63683553e-18 -3.63683553e-18 -3.63683553e-18]\n",
      "W[1]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[1]:\n",
      " [0.1 0.1 0.1 0.1]\n",
      "W[2]:\n",
      " [[0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1]]\n",
      "b[2]:\n",
      " [0.1 0.1 0.1]\n",
      "W[3]:\n",
      " [[0.09289322 0.09289322 0.09289322]\n",
      " [0.11421357 0.11421357 0.11421357]\n",
      " [0.09289322 0.09289322 0.09289322]]\n",
      "b[3]:\n",
      " [0.06666667 0.16666667 0.06666667]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "L = len(units) - 1\n",
    "\n",
    "# Initialize weights with correct shapes\n",
    "W = [None]\n",
    "b = [None]\n",
    "\n",
    "for l in range(1, L+1):\n",
    "    W.append(np.ones((units[l], units[l-1])) * 0.1)\n",
    "    print(f\"W[{l}]:\\n\", W[l])\n",
    "    b.append(np.ones((units[l])) * 0.1)\n",
    "    print(f\"b[{l}]:\\n\", b[l])\n",
    "\n",
    "# Forward Pass\n",
    "z = [None]\n",
    "h = [inputs[0]]\n",
    "print(f\"h[0] {h[0].shape}:\\n\", h[0])\n",
    "\n",
    "for l in range(1, len(units)):\n",
    "    z.append(W[l] @ h[l-1] + b[l])\n",
    "    print(f\"z[{l}] {z[l].shape}:\\n\", z[l])\n",
    "\n",
    "    if l == L:\n",
    "        break\n",
    "\n",
    "    h.append(np.tanh(z[l]))\n",
    "    print(f\"h[{l}] {h[l].shape}:\\n\", h[l])\n",
    "\n",
    "p = np.exp(z[L]) / np.sum(np.exp(z[L]))\n",
    "y = labels[0]\n",
    "\n",
    "# Loss\n",
    "loss = - y @ np.log(p) # - y @ z[L] + np.log(np.sum(np.exp(z[L])))\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Backpropagation\n",
    "dL_dz = [None] * (L+1)\n",
    "dL_dW = [None] * (L+1)\n",
    "dL_db = [None] * (L+1)\n",
    "dL_dh = [None] * L\n",
    "\n",
    "for l in range(L, 0, -1):\n",
    "    if l == L:\n",
    "        dL_dz[L] = -y + p\n",
    "    \n",
    "    else:\n",
    "        dL_dh[l] = W[l+1].T @ dL_dz[l+1]\n",
    "        print(f\"dL/dh[{l}] {dL_dh[l].shape}:\\n\", dL_dh[l])\n",
    "\n",
    "        dL_dz[l] = dL_dh[l] * (1 - h[l]**2)\n",
    "    print(f\"dL/dz[{l}] {dL_dz[l].shape}:\\n\", dL_dz[l])\n",
    "\n",
    "    dL_dW[l] = np.outer(dL_dz[l], h[l-1])\n",
    "    print(f\"dL/dW[{l}] {dL_dW[l].shape}:\\n\", dL_dW[l])\n",
    "\n",
    "    dL_db[l] = dL_dz[l]\n",
    "    print(f\"dL/db[{l}] {dL_db[l].shape}:\\n\", dL_db[l])\n",
    "\n",
    "# Update Gradients\n",
    "eta = 0.1\n",
    "for l in range(1, L+1):\n",
    "    W[l] -= eta * dL_dW[l]\n",
    "    print(f\"W[{l}]:\\n\", W[l])\n",
    "\n",
    "    b[l] -= eta * dL_db[l]\n",
    "    print(f\"b[{l}]:\\n\", b[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "\n",
    "    # compute hidden layers\n",
    "    for i in range(num_layers):\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        z = weights[i] @ h + biases[i]\n",
    "        if i < num_layers - 1:\n",
    "            hiddens.append(g(z))\n",
    "\n",
    "    #compute output\n",
    "    output = z\n",
    "\n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # compute loss\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -y @ np.log(probs)\n",
    "    return loss\n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y\n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations\n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        # Gradient of hidden parameters.\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        grad_weights.append(np.outer(grad_z, h))\n",
    "        grad_biases.append(grad_z)\n",
    "\n",
    "        # Gradient of hidden layer below\n",
    "        grad_h = weights[i].T @ grad_z\n",
    "\n",
    "        # Gradient of hidden layer below before activation\n",
    "        grad_z = grad_h * (1 - h**2)\n",
    "\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data\n",
    "labels = data.target\n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        # Compute Loss and update total loss\n",
    "        loss = compute_loss(output, y)\n",
    "        total_loss += loss\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        # Update weights\n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= eta * grad_weights[i]\n",
    "            biases[i] -= eta * grad_biases[i]\n",
    "\n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "units = [p, 50, n_classes]\n",
    "weights = [0.1 * np.random.randn(units[i+1], units[i]) for i in range(len(units) - 1)]\n",
    "biases = [0.1 * np.random.randn(units[i+1]) for i in range(len(units) - 1)]\n",
    "# Empty loss list\n",
    "losses = []\n",
    "# Learning rate.\n",
    "eta = 0.001\n",
    "# Run epochs and append loss to list\n",
    "for epoch in range(100):\n",
    "    weights, biases, loss = MLP_train_epoch(X_train, y_train_ohe, weights, biases)\n",
    "    losses.append(loss)\n",
    "# Plot loss evolution\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass and get the class with the highest probability\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        y_hat = np.argmax(output)\n",
    "        predicted_labels.append(y_hat)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = MLP_predict(X_train, weights, biases)\n",
    "y_test_pred = MLP_predict(X_test, weights, biases)\n",
    "\n",
    "print(f'Train accuracy: {(y_train_pred==y_train).mean()}')\n",
    "print(f'Test accuracy: {(y_test_pred==y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
